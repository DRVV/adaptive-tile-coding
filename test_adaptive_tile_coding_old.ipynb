{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.9, -0.7, -0.5, -0.3, -0.1,  0.1,  0.3,  0.5,  0.7]),\n",
       " array([-3.5, -2.5, -1.5, -0.5,  0.5,  1.5,  2.5,  3.5,  4.5])]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_tiling_grid(low, high, bins=(10, 10), offsets=(0.0, 0.0)):\n",
    "    \"\"\"Define a uniformly-spaced grid that can be used for tile-coding a space.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    low : array_like\n",
    "        Lower bounds for each dimension of the continuous space.\n",
    "    high : array_like\n",
    "        Upper bounds for each dimension of the continuous space.\n",
    "    bins : tuple\n",
    "        Number of bins or tiles along each corresponding dimension.\n",
    "    offsets : tuple\n",
    "        Split points for each dimension should be offset by these values.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    grid : list of array_like\n",
    "        A list of arrays containing split points for each dimension.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this\n",
    "    dim = len(low)\n",
    "    assert dim == len(high)\n",
    "    assert dim == len(offsets)\n",
    "    \n",
    "    low = np.array(low)\n",
    "    high = np.array(high)\n",
    "    bins = np.array(bins)\n",
    "    \n",
    "    bins = bins + 1\n",
    "    return [(np.linspace(start=low[i], stop=high[i], num=bins[i], endpoint=True) + offsets[i])[1:-1] for i in range(dim)]\n",
    "        \n",
    "\n",
    "\n",
    "low = [-1.0, -5.0]\n",
    "high = [1.0, 5.0]\n",
    "create_tiling_grid(low, high, bins=(10, 10), offsets=(-0.1, 0.5))  # [test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tilings(low, high, tiling_specs):\n",
    "    \"\"\"Define multiple tilings using the provided specifications.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    low : array_like\n",
    "        Lower bounds for each dimension of the continuous space.\n",
    "    high : array_like\n",
    "        Upper bounds for each dimension of the continuous space.\n",
    "    tiling_specs : list of tuples\n",
    "        A sequence of (bins, offsets) to be passed to create_tiling_grid().\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tilings : list\n",
    "        A list of tilings (grids), each produced by create_tiling_grid().\n",
    "    \"\"\"\n",
    "    # TODO: Implement this\n",
    "    grids = []\n",
    "    for spec in tiling_specs:\n",
    "        bins, offsets = spec        \n",
    "        grid = create_tiling_grid(low, high, bins, offsets)\n",
    "        grids.append(grid)\n",
    "        \n",
    "    return grids\n",
    "\n",
    "\n",
    "# Tiling specs: [(<bins>, <offsets>), ...]\n",
    "tiling_specs = [((10, 10), (-0.066, -0.33)),\n",
    "                ((10, 10), (0.0, 0.0)),\n",
    "                ((10, 10), (0.066, 0.33))]\n",
    "tilings = create_tilings(low, high, tiling_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Samples:\n",
      "[(-1.2, -5.1), (-0.75, 3.25), (-0.5, 0.0), (0.25, -1.9), (0.15, -1.75), (0.75, 2.5), (0.7, -3.7), (1.0, 5.0)]\n",
      "\n",
      "Encoded samples:\n",
      "[[(0, 0), (0, 0), (0, 0)], [(1, 8), (1, 8), (0, 7)], [(2, 5), (2, 5), (2, 4)], [(6, 3), (6, 3), (5, 2)], [(6, 3), (5, 3), (5, 2)], [(9, 7), (8, 7), (8, 7)], [(8, 1), (8, 1), (8, 0)], [(9, 9), (9, 9), (9, 9)]]\n"
     ]
    }
   ],
   "source": [
    "def discretize(sample, grid):\n",
    "    \"\"\"Discretize a sample as per given grid.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sample : array_like\n",
    "        A single sample from the (original) continuous space.\n",
    "    grid : list of array_like\n",
    "        A list of arrays containing split points for each dimension.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    discretized_sample : array_like\n",
    "        A sequence of integers with the same number of dimensions as sample.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this\n",
    "    return tuple(int(np.digitize(s, g)) for s, g in zip(sample, grid))  # apply along each dimension\n",
    "\n",
    "\n",
    "def tile_encode(sample, tilings, flatten=False):\n",
    "    \"\"\"Encode given sample using tile-coding.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sample : array_like\n",
    "        A single sample from the (original) continuous space.\n",
    "    tilings : list\n",
    "        A list of tilings (grids), each produced by create_tiling_grid().\n",
    "    flatten : bool\n",
    "        If true, flatten the resulting binary arrays into a single long vector.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    encoded_sample : list or array_like\n",
    "        A list of binary vectors, one for each tiling, or flattened into one.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this\n",
    "    encoded_sample = [discretize(sample, grid) for grid in tilings]\n",
    "    return np.concatenate(encoded_sample) if flatten else encoded_sample\n",
    "\n",
    "\n",
    "# Test with some sample values\n",
    "samples = [(-1.2 , -5.1 ),\n",
    "           (-0.75,  3.25),\n",
    "           (-0.5 ,  0.0 ),\n",
    "           ( 0.25, -1.9 ),\n",
    "           ( 0.15, -1.75),\n",
    "           ( 0.75,  2.5 ),\n",
    "           ( 0.7 , -3.7 ),\n",
    "           ( 1.0 ,  5.0 )]\n",
    "encoded_samples = [tile_encode(sample, tilings) for sample in samples]\n",
    "print(\"\\nSamples:\", repr(samples), sep=\"\\n\")\n",
    "print(\"\\nEncoded samples:\", repr(encoded_samples), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QTable(): size = (10, 10, 2)\n",
      "QTable(): size = (10, 10, 2)\n",
      "QTable(): size = (10, 10, 2)\n",
      "TiledQTable(): no. of internal tables =  3\n",
      "[GET]    Q((0.25, -1.9), 0) = 0.0\n",
      "[UPDATE] Q((0.15, -1.75), 0) = 1.0\n",
      "[GET]    Q((0.25, -1.9), 0) = 0.06666666666666667\n"
     ]
    }
   ],
   "source": [
    "class QTable:\n",
    "    \"\"\"Simple Q-table.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "        \"\"\"Initialize Q-table.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state_size : tuple\n",
    "            Number of discrete values along each dimension of state space.\n",
    "        action_size : int\n",
    "            Number of discrete actions in action space.\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # TODO: Create Q-table, initialize all Q-values to zero\n",
    "        # Note: If state_size = (9, 9), action_size = 2, q_table.shape should be (9, 9, 2)\n",
    "        self.q_table = np.zeros(shape=(self.state_size + (self.action_size,)))\n",
    "        print(\"QTable(): size =\", self.q_table.shape)\n",
    "\n",
    "\n",
    "class TiledQTable:\n",
    "    \"\"\"Composite Q-table with an internal tile coding scheme.\"\"\"\n",
    "    \n",
    "    def __init__(self, low, high, tiling_specs, action_size):\n",
    "        \"\"\"Create tilings and initialize internal Q-table(s).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        low : array_like\n",
    "            Lower bounds for each dimension of state space.\n",
    "        high : array_like\n",
    "            Upper bounds for each dimension of state space.\n",
    "        tiling_specs : list of tuples\n",
    "            A sequence of (bins, offsets) to be passed to create_tilings() along with low, high.\n",
    "        action_size : int\n",
    "            Number of discrete actions in action space.\n",
    "        \"\"\"\n",
    "        self.tilings = create_tilings(low, high, tiling_specs)\n",
    "        self.state_sizes = [tuple(len(splits)+1 for splits in tiling_grid) for tiling_grid in self.tilings]\n",
    "        self.action_size = action_size\n",
    "        self.q_tables = [QTable(state_size, self.action_size) for state_size in self.state_sizes]\n",
    "        print(\"TiledQTable(): no. of internal tables = \", len(self.q_tables))\n",
    "    \n",
    "    def get(self, state, action):\n",
    "        \"\"\"Get Q-value for given <state, action> pair.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state : array_like\n",
    "            Vector representing the state in the original continuous space.\n",
    "        action : int\n",
    "            Index of desired action.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        value : float\n",
    "            Q-value of given <state, action> pair, averaged from all internal Q-tables.\n",
    "        \"\"\"\n",
    "        # TODO: Encode state to get tile indices\n",
    "        encoded_state = tile_encode(state, self.tilings)\n",
    "        \n",
    "        # TODO: Retrieve q-value for each tiling, and return their average\n",
    "        value = 0.0\n",
    "        for idx, q_table in zip(encoded_state, self.q_tables):\n",
    "            value += q_table.q_table[tuple(idx + (action,))]\n",
    "        value /= len(self.q_tables)\n",
    "        return value\n",
    "    \n",
    "    def update(self, state, action, value, alpha=0.1):\n",
    "        \"\"\"Soft-update Q-value for given <state, action> pair to value.\n",
    "        \n",
    "        Instead of overwriting Q(state, action) with value, perform soft-update:\n",
    "            Q(state, action) = alpha * value + (1.0 - alpha) * Q(state, action)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state : array_like\n",
    "            Vector representing the state in the original continuous space.\n",
    "        action : int\n",
    "            Index of desired action.\n",
    "        value : float\n",
    "            Desired Q-value for <state, action> pair.\n",
    "        alpha : float\n",
    "            Update factor to perform soft-update, in [0.0, 1.0] range.\n",
    "        \"\"\"\n",
    "        # TODO: Encode state to get tile indices\n",
    "        encoded_state = tile_encode(state, self.tilings)\n",
    "        \n",
    "        # TODO: Update q-value for each tiling by update factor alpha\n",
    "        for idx, q_table in zip(encoded_state, self.q_tables):\n",
    "            value_ = q_table.q_table[tuple(idx + (action,))]  # current value\n",
    "            q_table.q_table[tuple(idx + (action,))] = alpha * value + (1.0 - alpha) * value_\n",
    "\n",
    "\n",
    "# Test with a sample Q-table\n",
    "tq = TiledQTable(low, high, tiling_specs, 2)\n",
    "s1 = 3; s2 = 4; a = 0; q = 1.0\n",
    "print(\"[GET]    Q({}, {}) = {}\".format(samples[s1], a, tq.get(samples[s1], a)))  # check value at sample = s1, action = a\n",
    "print(\"[UPDATE] Q({}, {}) = {}\".format(samples[s2], a, q)); tq.update(samples[s2], a, q)  # update value for sample with some common tile(s)\n",
    "print(\"[GET]    Q({}, {}) = {}\".format(samples[s1], a, tq.get(samples[s1], a)))  # check value again, should be slightly updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"Q-Learning agent that can act on a continuous state space by discretizing it.\"\"\"\n",
    "\n",
    "    def __init__(self, env, tq, alpha=0.02, gamma=0.99,\n",
    "                 epsilon=1.0, epsilon_decay_rate=0.9995, min_epsilon=.01, seed=0):\n",
    "        \"\"\"Initialize variables, create grid for discretization.\"\"\"\n",
    "        # Environment info\n",
    "        self.env = env\n",
    "        self.tq = tq \n",
    "        self.state_sizes = tq.state_sizes           # list of state sizes for each tiling\n",
    "        self.action_size = self.env.action_space.n  # 1-dimensional discrete action space\n",
    "        self.seed = np.random.seed(seed)\n",
    "        print(\"Environment:\", self.env)\n",
    "        print(\"State space sizes:\", self.state_sizes)\n",
    "        print(\"Action space size:\", self.action_size)\n",
    "        \n",
    "        # Learning parameters\n",
    "        self.alpha = alpha  # learning rate\n",
    "        self.gamma = gamma  # discount factor\n",
    "        self.epsilon = self.initial_epsilon = epsilon  # initial exploration rate\n",
    "        self.epsilon_decay_rate = epsilon_decay_rate   # how quickly should we decrease epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "\n",
    "    def reset_episode(self, state):\n",
    "        \"\"\"Reset variables for a new episode.\"\"\"\n",
    "        # Gradually decrease exploration rate\n",
    "        self.epsilon *= self.epsilon_decay_rate\n",
    "        self.epsilon = max(self.epsilon, self.min_epsilon)\n",
    "        \n",
    "        self.last_state = state\n",
    "        Q_s = [self.tq.get(state, action) for action in range(self.action_size)]\n",
    "        self.last_action = np.argmax(Q_s)\n",
    "        return self.last_action\n",
    "    \n",
    "    def reset_exploration(self, epsilon=None):\n",
    "        \"\"\"Reset exploration rate used when training.\"\"\"\n",
    "        self.epsilon = epsilon if epsilon is not None else self.initial_epsilon\n",
    "\n",
    "    def act(self, state, reward=None, done=None, mode='train'):\n",
    "        \"\"\"Pick next action and update internal Q table (when mode != 'test').\"\"\"\n",
    "        Q_s = [self.tq.get(state, action) for action in range(self.action_size)]\n",
    "        # Pick the best action from Q table\n",
    "        greedy_action = np.argmax(Q_s)\n",
    "        if mode == 'test':\n",
    "            # Test mode: Simply produce an action\n",
    "            action = greedy_action\n",
    "        else:\n",
    "            # Train mode (default): Update Q table, pick next action\n",
    "            # Note: We update the Q table entry for the *last* (state, action) pair with current state, reward\n",
    "            value = reward + self.gamma * max(Q_s)\n",
    "            self.tq.update(self.last_state, self.last_action, value, self.alpha)\n",
    "\n",
    "            # Exploration vs. exploitation\n",
    "            do_exploration = np.random.uniform(0, 1) < self.epsilon\n",
    "            if do_exploration:\n",
    "                # Pick a random action\n",
    "                action = np.random.randint(0, self.action_size)\n",
    "            else:\n",
    "                # Pick the greedy action\n",
    "                action = greedy_action\n",
    "\n",
    "        # Roll over current state, action for next step\n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "        return action\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space: Box(6,)\n",
      "- low: [ -1.        -1.        -1.        -1.       -12.566371 -28.274334]\n",
      "- high: [ 1.        1.        1.        1.       12.566371 28.274334]\n",
      "Action space: Discrete(3)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "# Create an environment\n",
    "env = gym.make('Acrobot-v1')\n",
    "env.seed(505);\n",
    "\n",
    "# Explore state (observation) space\n",
    "print(\"State space:\", env.observation_space)\n",
    "print(\"- low:\", env.observation_space.low)\n",
    "print(\"- high:\", env.observation_space.high)\n",
    "\n",
    "# Explore action space\n",
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QTable(): size = (5, 5, 5, 5, 5, 5, 3)\n",
      "TiledQTable(): no. of internal tables =  1\n",
      "Environment: <TimeLimit<AcrobotEnv<Acrobot-v1>>>\n",
      "State space sizes: [(5, 5, 5, 5, 5, 5)]\n",
      "Action space size: 3\n"
     ]
    }
   ],
   "source": [
    "n_bins = 5\n",
    "bins = tuple([n_bins]*env.observation_space.shape[0])\n",
    "tiling_specs = [(bins, tuple([0.0]*env.observation_space.shape[0]))\n",
    "               ]\n",
    "\n",
    "tq = TiledQTable(env.observation_space.low, \n",
    "                 env.observation_space.high, \n",
    "                 tiling_specs, \n",
    "                 env.action_space.n)\n",
    "agent = QLearningAgent(env, tq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10000/10000 | Max Average Score: -266.0"
     ]
    }
   ],
   "source": [
    "def run(agent, env, num_episodes=10000, mode='train'):\n",
    "    \"\"\"Run agent in given reinforcement learning environment and return scores.\"\"\"\n",
    "    scores = []\n",
    "    max_avg_score = -np.inf\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # Initialize episode\n",
    "        state = env.reset()\n",
    "        action = agent.reset_episode(state)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        # Roll out steps until done\n",
    "        while not done:\n",
    "            state, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            action = agent.act(state, reward, done, mode)\n",
    "\n",
    "        # Save final score\n",
    "        scores.append(total_reward)\n",
    "\n",
    "        # Print episode stats\n",
    "        if mode == 'train':\n",
    "            if len(scores) > 100:\n",
    "                avg_score = np.mean(scores[-100:])\n",
    "                if avg_score > max_avg_score:\n",
    "                    max_avg_score = avg_score\n",
    "            if i_episode % 100 == 0:\n",
    "                print(\"\\rEpisode {}/{} | Max Average Score: {}\".format(i_episode, num_episodes, max_avg_score), end=\"\")\n",
    "                sys.stdout.flush()\n",
    "    return scores\n",
    "\n",
    "scores = run(agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
