{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "low = [-1.0, -5.0]\n",
    "high = [1.0, 5.0]\n",
    "# Test with some sample values\n",
    "samples = [(-1.2 , -5.1 ),\n",
    "           (-0.75,  3.25),\n",
    "           (-0.5 ,  0.0 ),\n",
    "           ( 0.25, -1.9 ),\n",
    "           ( 0.15, -1.75),\n",
    "           ( 0.75,  2.5 ),\n",
    "           ( 0.7 , -3.7 ),\n",
    "           ( 1.0 ,  5.0 )]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtree import DNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-dfe0c74d5b17>, line 160)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-dfe0c74d5b17>\"\u001b[0;36m, line \u001b[0;32m160\u001b[0m\n\u001b[0;31m    self.qtree[idx].left =\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class QTree:\n",
    "    \"\"\"Composite Q-table with an internal tile coding scheme.\"\"\"\n",
    "    \n",
    "    def __init__(self, lower, upper, split, action_size):\n",
    "        \"\"\"Create regular tile, implemented with tree structure\n",
    "        Parameters\n",
    "        ----------\n",
    "        lower: array-like\n",
    "            Lower bounds of grid for each dimension\n",
    "        upper: array-like\n",
    "            Upper bounds of grid for each dimension\n",
    "        split: int\n",
    "            number of cells for each dimension\n",
    "            TODO: same value for every dimension\n",
    "        action_size: int\n",
    "            number of admissible actions\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.state_sizes = len(lower)\n",
    "        self.qtree = self.regular_qtree(lower, upper, split, action_size)\n",
    "\n",
    "    def regular_qtree(self, lower, upper, split, action_size):\n",
    "        MAXHEIGHT = int(np.log2(split) - 1)\n",
    "        ACTIONSIZE = action_size\n",
    "\n",
    "        init_lower = np.array(lower)\n",
    "        init_upper = np.array(upper)\n",
    "        init_dim = 0\n",
    "        init_middle = (init_lower + init_upper) / 2\n",
    "        init_idx = 0\n",
    "        init_height = 0\n",
    "\n",
    "        DIM = len(lower)\n",
    "        assert len(lower) == len(upper) # state vector dimension should match\n",
    "\n",
    "        root = DNode(init_middle[init_dim], init_dim)\n",
    "\n",
    "        def reggrid_nd_q(idx, lower, upper, cur_height, n_dim):\n",
    "            middle = (lower + upper)/2\n",
    "\n",
    "            if cur_height > MAXHEIGHT:\n",
    "                if n_dim == DIM - 1:\n",
    "                    root[idx] = DNode(idx)\n",
    "                    root[idx].q = np.zeros(ACTIONSIZE)\n",
    "                    root[idx].abs_delta_q = np.inf\n",
    "                    root[idx].subweights = np.zeros(self.state_sizes)\n",
    "                    root[idx].lower = lower\n",
    "                    root[idx].upper = upper\n",
    "                    root[idx].subtrees = [DNode(middle[d], d) for d in range(DIM)]\n",
    "                    for d, node in enumerate(root[idx].subtrees):\n",
    "                        # zero initializ\n",
    "                        node.q = np.zeros(action_size)\n",
    "                    \n",
    "                    return\n",
    "                else:\n",
    "                    cur_height = 0\n",
    "                    n_dim += 1\n",
    "                    lower = init_lower\n",
    "                    upper = init_upper\n",
    "                    middle = (lower + upper) / 2\n",
    "                    root[idx] = DNode(middle[n_dim], n_dim)\n",
    "\n",
    "            if idx > 0:\n",
    "                root[idx] = DNode(middle[n_dim], n_dim)\n",
    "\n",
    "            reggrid_nd_q(2*idx+1, lower, middle, cur_height + 1, n_dim)\n",
    "            reggrid_nd_q(2*idx+2, middle, upper, cur_height + 1, n_dim)\n",
    "\n",
    "        reggrid_nd_q(init_idx, init_lower, init_upper, init_height, init_dim)\n",
    "        return root\n",
    "\n",
    "    def search_spacial(self, root, test_array): \n",
    "        \"\"\"binary search on decision tree\"\"\"\n",
    "        # Base Cases: root is null or key is present at root \n",
    "        if root is None or root.dim is None: \n",
    "            # returns q-value array, and its index of the node in the tree\n",
    "            return root.q, root.value\n",
    "\n",
    "        # Key is greater than root's key \n",
    "        if test_array[root.dim] < root.value: \n",
    "            return self.search_spacial(root.left, test_array)\n",
    "        else:\n",
    "            # Key is smaller than root's key \n",
    "            return self.search_spacial(root.right, test_array)\n",
    "        \n",
    "    def get(self, state, action, return_index=True):\n",
    "        \"\"\"Get Q-value for given <state, action> pair.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state: array_like\n",
    "            array representing the state in the original continuous space\n",
    "        action: int\n",
    "            Index or label of action\n",
    "        return_index: bool\n",
    "            returns index of the node if true\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        value: float\n",
    "            Q-value of given <state, action> pair\n",
    "        idx: int\n",
    "            index of the node in which the <state, action> pair falls\n",
    "        \"\"\"\n",
    "    \n",
    "        # TODO: parse state tree to get q-value array\n",
    "        qarray, idx = self.search_spacial(self.qtree, state)\n",
    "        # TODO: get q-value by accessing array\n",
    "        q_value = qarray[action]\n",
    "        if return_index:\n",
    "            return q_value, idx\n",
    "        else:\n",
    "            return q_value\n",
    "    \n",
    "    \n",
    "    def update(self, state, action, value, alpha=0.1):\n",
    "        \"\"\"Soft-update Q-value for given <state, action> pair to value.\n",
    "        \n",
    "        Instead of overwriting Q(state, action) with value, perform soft-update:\n",
    "            Q(state, action) = alpha * value + (1.0 - alpha) * Q(state, action)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state : array_like\n",
    "            Vector representing the state in the original continuous space.\n",
    "        action : int\n",
    "            Index of desired action.\n",
    "        value : float\n",
    "            Desired Q-value for <state, action> pair.\n",
    "        alpha : float\n",
    "            Update factor to perform soft-update, in [0.0, 1.0] range.\n",
    "        \"\"\"\n",
    "        # get current value and its reference\n",
    "        q_value_current, idx = self.get(state, action)\n",
    "        # update the value based on observed value\n",
    "        self.qtree[idx].q[action] = alpha * value + (1.0 - alpha) * q_value_current\n",
    "        \n",
    "        # update subtile weight\n",
    "        for d in range(self.state_sizes):\n",
    "            self.qtree[idx].subtrees[d]\n",
    "        \n",
    "#         for d in range(self.state_sizes):\n",
    "#             weight_current = self.qtree[idx].subweights[d]\n",
    "#             self.qtree[idx].subweights[d] = alpha * value + (1.0 - alpha) * weight_current\n",
    "        \n",
    "        self.qtree[idx].subweights = alpha * value + (1.0 - alpha) * self.qtree[idx].subweights\n",
    "        \n",
    "        # update lowest delta q\n",
    "        abs_delta_q = abs(value - q_value_current)\n",
    "        if abs_delta_q < self.qtree[idx].abs_delta_q:\n",
    "            self.qtree[idx].abs_delta_q = abs_delta_q\n",
    "            self.u = 0\n",
    "        else:\n",
    "            self.u += 1\n",
    "        \n",
    "        if self.u > self.p:\n",
    "            # split\n",
    "            self.qtree[idx].left = \n",
    "            # init u\n",
    "            self.u = 0\n",
    "            \n",
    "            \n",
    "        \n",
    "            \n",
    "        self.qtree[idx].delta_q = value - q_value_current\n",
    "        \n",
    "        if abs(delta_q) < \n",
    "        \n",
    "        # update tree if necessary\n",
    "        \n",
    "        \n",
    "\n",
    "# Test with a sample Q-table\n",
    "tq = QTree(low, high, 4, 2)\n",
    "s1 = 3; s2 = 4; a = 0; q = 1.0\n",
    "print(\"[GET]    Q({}, {}) = {}\".format(samples[s1], a, tq.get(samples[s1], a)))  # check value at sample = s1, action = a\n",
    "print(\"[UPDATE] Q({}, {}) = {}\".format(samples[s2], a, q)); tq.update(samples[s2], a, q)  # update value for sample with some common tile(s)\n",
    "print(\"[GET]    Q({}, {}) = {}\".format(samples[s1], a, tq.get(samples[s1], a)))  # check value again, should be slightly updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GET]    Q((0.15, -1.75), 0) = (0.1, 24)\n"
     ]
    }
   ],
   "source": [
    "print(\"[GET]    Q({}, {}) = {}\".format(samples[s2], a, tq.get(samples[s2], a)))  # check value again, should be slightly updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"Q-Learning agent that can act on a continuous state space by discretizing it.\"\"\"\n",
    "\n",
    "    def __init__(self, env, tq, alpha=0.02, gamma=0.99,\n",
    "                 epsilon=1.0, epsilon_decay_rate=0.9995, min_epsilon=.01, seed=0):\n",
    "        \"\"\"Initialize variables, create grid for discretization.\"\"\"\n",
    "        # Environment info\n",
    "        self.env = env\n",
    "        self.tq = tq \n",
    "        self.state_sizes = tq.state_sizes           # list of state sizes for each tiling\n",
    "        self.action_size = self.env.action_space.n  # 1-dimensional discrete action space\n",
    "        self.seed = np.random.seed(seed)\n",
    "        print(\"Environment:\", self.env)\n",
    "        print(\"State space sizes:\", self.state_sizes)\n",
    "        print(\"Action space size:\", self.action_size)\n",
    "        \n",
    "        # Learning parameters\n",
    "        self.alpha = alpha  # learning rate\n",
    "        self.gamma = gamma  # discount factor\n",
    "        self.epsilon = self.initial_epsilon = epsilon  # initial exploration rate\n",
    "        self.epsilon_decay_rate = epsilon_decay_rate   # how quickly should we decrease epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "\n",
    "    def reset_episode(self, state):\n",
    "        \"\"\"Reset variables for a new episode.\"\"\"\n",
    "        # Gradually decrease exploration rate\n",
    "        self.epsilon *= self.epsilon_decay_rate\n",
    "        self.epsilon = max(self.epsilon, self.min_epsilon)\n",
    "        \n",
    "        self.last_state = state\n",
    "        Q_s = [self.tq.get(state, action, return_index=False) for action in range(self.action_size)]\n",
    "        self.last_action = np.argmax(Q_s)\n",
    "        return self.last_action\n",
    "    \n",
    "    def reset_exploration(self, epsilon=None):\n",
    "        \"\"\"Reset exploration rate used when training.\"\"\"\n",
    "        self.epsilon = epsilon if epsilon is not None else self.initial_epsilon\n",
    "\n",
    "    def act(self, state, reward=None, done=None, mode='train'):\n",
    "        \"\"\"Pick next action and update internal Q table (when mode != 'test').\"\"\"\n",
    "        Q_s = [self.tq.get(state, action, return_index=False) for action in range(self.action_size)]\n",
    "        # Pick the best action from Q table\n",
    "        greedy_action = np.argmax(Q_s)\n",
    "        if mode == 'test':\n",
    "            # Test mode: Simply produce an action\n",
    "            action = greedy_action\n",
    "        else:\n",
    "            # Train mode (default): Update Q table, pick next action\n",
    "            # Note: We update the Q table entry for the *last* (state, action) pair with current state, reward\n",
    "            value = reward + self.gamma * max(Q_s)\n",
    "            self.tq.update(self.last_state, self.last_action, value, self.alpha)\n",
    "            \n",
    "#            self.tq.update_subtile_weight(self.last_state, self.last_action, value, self.slpha)\n",
    "#            self.tq.split_tile()\n",
    "            \n",
    "            # Exploration vs. exploitation\n",
    "            do_exploration = np.random.uniform(0, 1) < self.epsilon\n",
    "            if do_exploration:\n",
    "                # Pick a random action\n",
    "                action = np.random.randint(0, self.action_size)\n",
    "            else:\n",
    "                # Pick the greedy action\n",
    "                action = greedy_action\n",
    "\n",
    "        # Roll over current state, action for next step\n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "        return action\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space: Box(6,)\n",
      "- low: [ -1.        -1.        -1.        -1.       -12.566371 -28.274334]\n",
      "- high: [ 1.        1.        1.        1.       12.566371 28.274334]\n",
      "Action space: Discrete(3)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "# Create an environment\n",
    "env = gym.make('Acrobot-v1')\n",
    "env.seed(505);\n",
    "\n",
    "# Explore state (observation) space\n",
    "print(\"State space:\", env.observation_space)\n",
    "print(\"- low:\", env.observation_space.low)\n",
    "print(\"- high:\", env.observation_space.high)\n",
    "\n",
    "# Explore action space\n",
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: <TimeLimit<AcrobotEnv<Acrobot-v1>>>\n",
      "State space sizes: 6\n",
      "Action space size: 3\n"
     ]
    }
   ],
   "source": [
    "n_bins = 2\n",
    "#bins = tuple([n_bins]*env.observation_space.shape[0])\n",
    "#tiling_specs = [(bins, tuple([0.0]*env.observation_space.shape[0]))\n",
    "#               ]\n",
    "\n",
    "# tq = TiledQTable(env.observation_space.low, \n",
    "#                  env.observation_space.high, \n",
    "#                  tiling_specs, \n",
    "#                  env.action_space.n)\n",
    "tq = QTree(env.observation_space.low, env.observation_space.high, n_bins, env.action_space.n)\n",
    "agent = QLearningAgent(env, tq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8500/10000 | Max Average Score: -103.39"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def run(agent, env, num_episodes=10000, mode='train'):\n",
    "    \"\"\"Run agent in given reinforcement learning environment and return scores.\"\"\"\n",
    "    scores = []\n",
    "    max_avg_score = -np.inf\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # Initialize episode\n",
    "        state = env.reset()\n",
    "        action = agent.reset_episode(state)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        # Roll out steps until done\n",
    "        while not done:\n",
    "            state, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            action = agent.act(state, reward, done, mode)\n",
    "\n",
    "        # Save final score\n",
    "        scores.append(total_reward)\n",
    "\n",
    "        # Print episode stats\n",
    "        if mode == 'train':\n",
    "            if len(scores) > 100:\n",
    "                avg_score = np.mean(scores[-100:])\n",
    "                if avg_score > max_avg_score:\n",
    "                    max_avg_score = avg_score\n",
    "            if i_episode % 100 == 0:\n",
    "                print(\"\\rEpisode {}/{} | Max Average Score: {}\".format(i_episode, num_episodes, max_avg_score), end=\"\")\n",
    "                sys.stdout.flush()\n",
    "    return scores\n",
    "\n",
    "scores = run(agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
