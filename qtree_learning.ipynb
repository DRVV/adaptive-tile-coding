{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "low = [-1.0, -5.0]\n",
    "high = [1.0, 5.0]\n",
    "# Test with some sample values\n",
    "samples = [(-1.2 , -5.1 ),\n",
    "           (-0.75,  3.25),\n",
    "           (-0.5 ,  0.0 ),\n",
    "           ( 0.25, -1.9 ),\n",
    "           ( 0.15, -1.75),\n",
    "           ( 0.75,  2.5 ),\n",
    "           ( 0.7 , -3.7 ),\n",
    "           ( 1.0 ,  5.0 )]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtree import DNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GET]    Q((0.25, -1.9), 0) = (0.0, 24)\n",
      "[UPDATE] Q((0.15, -1.75), 0) = 1.0\n",
      "[GET]    Q((0.25, -1.9), 0) = (0.1, 24)\n"
     ]
    }
   ],
   "source": [
    "class QTree:\n",
    "    \"\"\"Composite Q-table with an internal tile coding scheme.\"\"\"\n",
    "    \n",
    "    def __init__(self, lower, upper, split, action_size):\n",
    "        \"\"\"Create regular tile, implemented with tree structure\n",
    "        Parameters\n",
    "        ----------\n",
    "        lower: array-like\n",
    "            Lower bounds of grid for each dimension\n",
    "        upper: array-like\n",
    "            Upper bounds of grid for each dimension\n",
    "        split: int\n",
    "            number of cells for each dimension\n",
    "            TODO: same value for every dimension\n",
    "        action_size: int\n",
    "            number of admissible actions\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.state_sizes = len(lower)\n",
    "        self.qtree = self.regular_qtree(lower, upper, split, action_size)\n",
    "\n",
    "    def regular_qtree(self, lower, upper, split, action_size):\n",
    "        MAXHEIGHT = int(np.log2(split) - 1)\n",
    "        ACTIONSIZE = action_size\n",
    "\n",
    "        init_lower = np.array(lower)\n",
    "        init_upper = np.array(upper)\n",
    "        init_dim = 0\n",
    "        init_middle = (init_lower + init_upper) / 2\n",
    "        init_idx = 0\n",
    "        init_height = 0\n",
    "\n",
    "        DIM = len(lower)\n",
    "        assert len(lower) == len(upper) # state vector dimension should match\n",
    "\n",
    "        root = DNode(init_middle[init_dim], init_dim)\n",
    "\n",
    "        def reggrid_nd_q(idx, lower, upper, cur_height, n_dim):\n",
    "            middle = (lower + upper)/2\n",
    "\n",
    "            if cur_height > MAXHEIGHT:\n",
    "                if n_dim == DIM - 1:\n",
    "                    root[idx] = DNode(idx)\n",
    "                    root[idx].q = np.zeros(ACTIONSIZE)\n",
    "                    return\n",
    "                else:\n",
    "                    cur_height = 0\n",
    "                    n_dim += 1\n",
    "                    lower = init_lower\n",
    "                    upper = init_upper\n",
    "                    middle = (lower + upper) / 2\n",
    "                    root[idx] = DNode(middle[n_dim], n_dim)\n",
    "\n",
    "            if idx > 0:\n",
    "                root[idx] = DNode(middle[n_dim], n_dim)\n",
    "\n",
    "            reggrid_nd_q(2*idx+1, lower, middle, cur_height + 1, n_dim)\n",
    "            reggrid_nd_q(2*idx+2, middle, upper, cur_height + 1, n_dim)\n",
    "\n",
    "        reggrid_nd_q(init_idx, init_lower, init_upper, init_height, init_dim)\n",
    "        return root\n",
    "\n",
    "    def search_spacial(self, root, test_array): \n",
    "        \"\"\"binary search on decision tree\"\"\"\n",
    "        # Base Cases: root is null or key is present at root \n",
    "        if root is None or root.dim is None: \n",
    "            # returns q-value array, and its index of the node in the tree\n",
    "            return root.q, root.value\n",
    "\n",
    "        # Key is greater than root's key \n",
    "        if test_array[root.dim] < root.value: \n",
    "            return self.search_spacial(root.left, test_array)\n",
    "        else:\n",
    "            # Key is smaller than root's key \n",
    "            return self.search_spacial(root.right, test_array)\n",
    "        \n",
    "    def get(self, state, action, return_index=True):\n",
    "        \"\"\"Get Q-value for given <state, action> pair.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state: array_like\n",
    "            array representing the state in the original continuous space\n",
    "        action: int\n",
    "            Index or label of action\n",
    "        return_index: bool\n",
    "            returns index of the node if true\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        value: float\n",
    "            Q-value of given <state, action> pair\n",
    "        idx: int\n",
    "            index of the node in which the <state, action> pair falls\n",
    "        \"\"\"\n",
    "    \n",
    "        # TODO: parse state tree to get q-value array\n",
    "        qarray, idx = self.search_spacial(self.qtree, state)\n",
    "        # TODO: get q-value by accessing array\n",
    "        q_value = qarray[action]\n",
    "        if return_index:\n",
    "            return q_value, idx\n",
    "        else:\n",
    "            return q_value\n",
    "    \n",
    "    \n",
    "    def update(self, state, action, value, alpha=0.1):\n",
    "        \"\"\"Soft-update Q-value for given <state, action> pair to value.\n",
    "        \n",
    "        Instead of overwriting Q(state, action) with value, perform soft-update:\n",
    "            Q(state, action) = alpha * value + (1.0 - alpha) * Q(state, action)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state : array_like\n",
    "            Vector representing the state in the original continuous space.\n",
    "        action : int\n",
    "            Index of desired action.\n",
    "        value : float\n",
    "            Desired Q-value for <state, action> pair.\n",
    "        alpha : float\n",
    "            Update factor to perform soft-update, in [0.0, 1.0] range.\n",
    "        \"\"\"\n",
    "        # get current value and its reference\n",
    "        q_value_current, idx = self.get(state, action)\n",
    "        # update the value based on observed value\n",
    "        self.qtree[idx].q[action] = alpha * value + (1.0 - alpha) * q_value_current\n",
    "\n",
    "# Test with a sample Q-table\n",
    "tq = QTree(low, high, 4, 2)\n",
    "s1 = 3; s2 = 4; a = 0; q = 1.0\n",
    "print(\"[GET]    Q({}, {}) = {}\".format(samples[s1], a, tq.get(samples[s1], a)))  # check value at sample = s1, action = a\n",
    "print(\"[UPDATE] Q({}, {}) = {}\".format(samples[s2], a, q)); tq.update(samples[s2], a, q)  # update value for sample with some common tile(s)\n",
    "print(\"[GET]    Q({}, {}) = {}\".format(samples[s1], a, tq.get(samples[s1], a)))  # check value again, should be slightly updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GET]    Q((0.15, -1.75), 0) = (0.1, 24)\n"
     ]
    }
   ],
   "source": [
    "print(\"[GET]    Q({}, {}) = {}\".format(samples[s2], a, tq.get(samples[s2], a)))  # check value again, should be slightly updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.15, -1.75)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[s2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.25, -1.9)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[s1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tq.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                                       _____________________________________X0<0.0____________________________________\n",
      "                                      /                                                                               \\\n",
      "                  ________________X0<-0.5________________                                            ________________X0<0.5________________\n",
      "                 /                                       \\                                          /                                      \\\n",
      "        ______X1<0.0_____                         ______X1<0.0_____                        ______X1<0.0_____                        ______X1<0.0_____\n",
      "       /                 \\                       /                 \\                      /                 \\                      /                 \\\n",
      "  _X1<-2.5             _X1<2.5              _X1<-2.5             _X1<2.5             _X1<-2.5             _X1<2.5             _X1<-2.5             _X1<2.5\n",
      " /        \\           /       \\            /        \\           /       \\           /        \\           /       \\           /        \\           /       \\\n",
      "15         16        17        18         19         20        21        22        23         24        25        26        27         28        29        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tq.qtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TiledQTable' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b9aa21c8ba7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mAdaptiveTiledQTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTiledQTable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_subtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TiledQTable' is not defined"
     ]
    }
   ],
   "source": [
    "class AdaptiveTiledQTable(TiledQTable):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def update_subtile(self, state, action, value, alpha=0.1):\n",
    "        \"\"\"Soft-update Q-value for given <state, action> pair to value.\n",
    "        \n",
    "        Instead of overwriting Q(state, action) with value, perform soft-update:\n",
    "            Q(state, action) = alpha * value + (1.0 - alpha) * Q(state, action)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state : array_like\n",
    "            Vector representing the state in the original continuous space.\n",
    "        action : int\n",
    "            Index of desired action.\n",
    "        value : float\n",
    "            Desired Q-value for <state, action> pair.\n",
    "        alpha : float\n",
    "            Update factor to perform soft-update, in [0.0, 1.0] range.\n",
    "        \"\"\"\n",
    "        # TODO: Encode state to get tile indices\n",
    "        encoded_state = tile_encode(state, self.tilings)\n",
    "        \n",
    "        # TODO: Update q-value for each tiling by update factor alpha\n",
    "        for idx, q_table in zip(encoded_state, self.q_tables):\n",
    "            value_ = q_table.q_table[tuple(idx + (action,))]  # current value\n",
    "            q_table.q_table[tuple(idx + (action,))] = alpha * value + (1.0 - alpha) * value_\n",
    "\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def split_tile():\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"Q-Learning agent that can act on a continuous state space by discretizing it.\"\"\"\n",
    "\n",
    "    def __init__(self, env, tq, alpha=0.02, gamma=0.99,\n",
    "                 epsilon=1.0, epsilon_decay_rate=0.9995, min_epsilon=.01, seed=0):\n",
    "        \"\"\"Initialize variables, create grid for discretization.\"\"\"\n",
    "        # Environment info\n",
    "        self.env = env\n",
    "        self.tq = tq \n",
    "        self.state_sizes = tq.state_sizes           # list of state sizes for each tiling\n",
    "        self.action_size = self.env.action_space.n  # 1-dimensional discrete action space\n",
    "        self.seed = np.random.seed(seed)\n",
    "        print(\"Environment:\", self.env)\n",
    "        print(\"State space sizes:\", self.state_sizes)\n",
    "        print(\"Action space size:\", self.action_size)\n",
    "        \n",
    "        # Learning parameters\n",
    "        self.alpha = alpha  # learning rate\n",
    "        self.gamma = gamma  # discount factor\n",
    "        self.epsilon = self.initial_epsilon = epsilon  # initial exploration rate\n",
    "        self.epsilon_decay_rate = epsilon_decay_rate   # how quickly should we decrease epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "\n",
    "    def reset_episode(self, state):\n",
    "        \"\"\"Reset variables for a new episode.\"\"\"\n",
    "        # Gradually decrease exploration rate\n",
    "        self.epsilon *= self.epsilon_decay_rate\n",
    "        self.epsilon = max(self.epsilon, self.min_epsilon)\n",
    "        \n",
    "        self.last_state = state\n",
    "        Q_s = [self.tq.get(state, action, return_index=False) for action in range(self.action_size)]\n",
    "        self.last_action = np.argmax(Q_s)\n",
    "        return self.last_action\n",
    "    \n",
    "    def reset_exploration(self, epsilon=None):\n",
    "        \"\"\"Reset exploration rate used when training.\"\"\"\n",
    "        self.epsilon = epsilon if epsilon is not None else self.initial_epsilon\n",
    "\n",
    "    def act(self, state, reward=None, done=None, mode='train'):\n",
    "        \"\"\"Pick next action and update internal Q table (when mode != 'test').\"\"\"\n",
    "        Q_s = [self.tq.get(state, action, return_index=False) for action in range(self.action_size)]\n",
    "        # Pick the best action from Q table\n",
    "        greedy_action = np.argmax(Q_s)\n",
    "        if mode == 'test':\n",
    "            # Test mode: Simply produce an action\n",
    "            action = greedy_action\n",
    "        else:\n",
    "            # Train mode (default): Update Q table, pick next action\n",
    "            # Note: We update the Q table entry for the *last* (state, action) pair with current state, reward\n",
    "            value = reward + self.gamma * max(Q_s)\n",
    "            self.tq.update(self.last_state, self.last_action, value, self.alpha)\n",
    "            \n",
    "#            self.tq.update_subtile_weight(self.last_state, self.last_action, value, self.slpha)\n",
    "#            self.tq.split_tile()\n",
    "            \n",
    "            # Exploration vs. exploitation\n",
    "            do_exploration = np.random.uniform(0, 1) < self.epsilon\n",
    "            if do_exploration:\n",
    "                # Pick a random action\n",
    "                action = np.random.randint(0, self.action_size)\n",
    "            else:\n",
    "                # Pick the greedy action\n",
    "                action = greedy_action\n",
    "\n",
    "        # Roll over current state, action for next step\n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "        return action\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space: Box(6,)\n",
      "- low: [ -1.        -1.        -1.        -1.       -12.566371 -28.274334]\n",
      "- high: [ 1.        1.        1.        1.       12.566371 28.274334]\n",
      "Action space: Discrete(3)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "# Create an environment\n",
    "env = gym.make('Acrobot-v1')\n",
    "env.seed(505);\n",
    "\n",
    "# Explore state (observation) space\n",
    "print(\"State space:\", env.observation_space)\n",
    "print(\"- low:\", env.observation_space.low)\n",
    "print(\"- high:\", env.observation_space.high)\n",
    "\n",
    "# Explore action space\n",
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: <TimeLimit<AcrobotEnv<Acrobot-v1>>>\n",
      "State space sizes: 6\n",
      "Action space size: 3\n"
     ]
    }
   ],
   "source": [
    "n_bins = 2\n",
    "#bins = tuple([n_bins]*env.observation_space.shape[0])\n",
    "#tiling_specs = [(bins, tuple([0.0]*env.observation_space.shape[0]))\n",
    "#               ]\n",
    "\n",
    "# tq = TiledQTable(env.observation_space.low, \n",
    "#                  env.observation_space.high, \n",
    "#                  tiling_specs, \n",
    "#                  env.action_space.n)\n",
    "tq = QTree(env.observation_space.low, env.observation_space.high, 4, env.action_space.n)\n",
    "agent = QLearningAgent(env, tq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(agent, env, num_episodes=10000, mode='train'):\n",
    "    \"\"\"Run agent in given reinforcement learning environment and return scores.\"\"\"\n",
    "    scores = []\n",
    "    max_avg_score = -np.inf\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # Initialize episode\n",
    "        state = env.reset()\n",
    "        action = agent.reset_episode(state)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        # Roll out steps until done\n",
    "        while not done:\n",
    "            state, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            action = agent.act(state, reward, done, mode)\n",
    "\n",
    "        # Save final score\n",
    "        scores.append(total_reward)\n",
    "\n",
    "        # Print episode stats\n",
    "        if mode == 'train':\n",
    "            if len(scores) > 100:\n",
    "                avg_score = np.mean(scores[-100:])\n",
    "                if avg_score > max_avg_score:\n",
    "                    max_avg_score = avg_score\n",
    "            if i_episode % 100 == 0:\n",
    "                print(\"\\rEpisode {}/{} | Max Average Score: {}\".format(i_episode, num_episodes, max_avg_score), end=\"\")\n",
    "                sys.stdout.flush()\n",
    "    return scores\n",
    "\n",
    "scores = run(agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
